The Open Language Model (OLMo) is a new framework released by the Allen Institute of AI, providing full access to data, training code, models, and evaluation tools to enhance the study of language models. The initial release features four models at the 7B parameter scale and one at 1B, all trained on over 2 trillion tokens, with plans for a 65B model in the future. Key components include the training data and code available on GitHub, as well as the Dolma dataset, a diverse corpus of 3 trillion tokens from 5 billion documents. The OLMo-7B and OLMo-1B models utilize a decoder-only transformer architecture with several enhancements, such as no biases and a vocabulary of 50,280. Evaluation results show that OLMo-7B outperforms other models on multiple tasks, particularly in commonsense reasoning, as assessed using the Catwalk evaluation suite with datasets like `piqa` and `hellaswag`. Further details and prompting guides are forthcoming.