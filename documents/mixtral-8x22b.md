Mixtral 8x22B is a new open large language model (LLM) from Mistral AI, featuring a sparse mixture-of-experts architecture with 39B active parameters out of 141B total. It is designed for cost efficiency and offers capabilities such as multilingual understanding, math reasoning, code generation, native function calling, and constrained output support, with a context window size of 64K tokens for effective information recall. Mistral AI claims it has one of the best performance-to-cost ratios among community models, demonstrating significant speed due to sparse activations. The model outperforms state-of-the-art open models like Command R+ and Llama 2 70B on various reasoning and knowledge benchmarks, as well as excelling in coding and math tasks, achieving a score of 90% on GSM8K. More details and usage instructions can be found at "https://docs.mistral.ai/getting-started/open_weight_models/#operation/listModels", and it is released under an Apache 2.0 license.