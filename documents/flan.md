The paper on scaling instruction-finetuned language models highlights the advantages of scaling instruction finetuning, which enhances performance across various models (like PaLM and T5), prompting setups (zero-shot, few-shot, CoT), and benchmarks (MMLU, TyDiQA). Key findings include that instruction finetuning benefits from increasing the number of tasks (1.8K tasks) and model size, with diminishing returns on task scaling. Notably, adding chain-of-thought (CoT) datasets improves reasoning task performance, as seen in Flan-PaLM's 14.9% improvement on one-shot TyDiQA and 8.1% on arithmetic reasoning in under-represented languages. Joint finetuning on non-CoT and CoT data enhances evaluations, and CoT finetuning enables zero-shot reasoning, exemplified by the phrase "let's think step-by-step" on BIG-Bench tasks. The Flan-PaLM model demonstrates superior zero-shot capabilities compared to PaLM, effectively handling challenging open-ended questions, while few-shot exemplars help mitigate errors in zero-shot settings. For practical applications, Flan-T5 models are available on the Hugging Face Hub.