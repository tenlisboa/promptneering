Zhang et al. (2023) introduced a multimodal chain-of-thought (CoT) prompting approach that integrates text and vision within a two-stage framework, contrasting with traditional CoT that focuses solely on language. The first stage involves generating rationales from multimodal information, followed by answer inference using these rationales. The multimodal CoT model (1B) has demonstrated superior performance over GPT-3.5 on the ScienceQA benchmark. For further exploration, see "Language Is Not All You Need: Aligning Perception with Language Models."