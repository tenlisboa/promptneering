Trustworthy large language models (LLMs) are crucial for applications in high-stakes fields like health and finance, yet they often lack guaranteed reliability in areas such as truthfulness, safety, and privacy. A recent study by Sun et al. (2024) outlines a framework for evaluating LLM trustworthiness across eight dimensions, with a benchmark focusing on six key aspects: truthfulness, safety, fairness, robustness, privacy, and machine ethics. The evaluation of 16 mainstream LLMs revealed that while proprietary models generally outperform open-source ones, models like GPT-4 and Llama 2 show significant resilience against adversarial attacks and can reject stereotypical statements. Key insights include that LLMs struggle with truthfulness due to data noise, open-source models lag in safety, and most LLMs perform poorly in recognizing stereotypes. Privacy handling varies, with some models leaking information, and while LLMs grasp basic moral principles, they falter in complex ethical situations. A leaderboard for LLM trustworthiness is available, and a GitHub repository provides an evaluation kit for testing these dimensions, with code accessible at "https://github.com/HowieHwong/TrustLLM".