A new paper by Google introduces a technique called Infini-attention, which integrates compressive memory into a standard dot-product attention layer to enable Transformer LLMs to process infinitely long inputs while maintaining a bounded memory footprint and computation. This method combines masked local attention and long-term linear attention within a single Transformer block, allowing the Infini-Transformer model to efficiently manage both long and short-range contextual dependencies. The approach demonstrates significant improvements over baseline models in long-context language modeling, achieving a 114x memory compression ratio. Notably, a 1B LLM can scale to a 1M sequence length, and an 8B model sets a new state-of-the-art result on a 500K length book summarization task, highlighting the potential of effective memory systems in enhancing reasoning, planning, and adaptability in LLMs.