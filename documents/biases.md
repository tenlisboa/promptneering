LLMs can exhibit harmful biases that may affect their performance on downstream tasks, which can sometimes be mitigated through effective prompting or require advanced solutions like moderation. When testing the distribution of exemplars in few-shot learning, it was found that a balanced distribution of examples is crucial; for instance, using the prompt with multiple positive and negative statements led to a consistent output of "Negative" when the distribution was skewed. Conversely, flipping the distribution resulted in a "Positive" response, indicating that the model's knowledge of sentiment classification can influence outcomes. Additionally, the order of exemplars can also impact model performance, so it is recommended to randomize the order to avoid bias, especially when the distribution of labels is uneven.