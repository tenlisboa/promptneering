Active-Prompt is a new prompting approach proposed by Diao et al. (2023) to enhance the adaptability of large language models (LLMs) to task-specific examples, addressing the limitations of fixed human-annotated exemplars in chain-of-thought (CoT) methods. The process begins by querying the LLM with or without CoT examples, generating *k* possible answers for training questions, and calculating an uncertainty metric based on the disagreement among these answers. The questions with the highest uncertainty are then selected for human annotation, and the newly annotated exemplars are utilized to infer responses for each question.