The Tree of Thoughts (ToT) framework, proposed by Yao et al. (2023) and Long (2023), enhances problem-solving capabilities in language models (LMs) by maintaining a tree of coherent thoughts that serve as intermediate steps. This approach allows LMs to self-evaluate their progress and systematically explore thoughts using search algorithms like breadth-first search (BFS) and depth-first search (DFS). For example, in the Game of 24 task, thoughts are decomposed into three steps, with the best five candidates evaluated as "sure/maybe/impossible" to reach the target. The ToT framework significantly outperforms traditional prompting methods. While both studies aim to improve LLMs through tree search, Yao et al. utilize generic search strategies, whereas Long's approach employs a "ToT Controller" trained via reinforcement learning for adaptability. Additionally, Hulbert (2023) introduced Tree-of-Thought Prompting, a simpler technique for evaluating intermediate thoughts in a single prompt, and Sun (2023) benchmarked this method with large-scale experiments, introducing the concept of PanelGPT for discussions among LLMs.