Prompt chaining is a technique in prompt engineering that enhances the performance and reliability of large language models (LLMs) by breaking complex tasks into subtasks, allowing for a series of prompt operations. This method improves transparency, controllability, and debugging capabilities in LLM applications, making it particularly beneficial for conversational assistants. A common use case involves document question answering, where two prompts are designed: the first extracts relevant quotes from a document based on a question, using a prompt like ```You are a helpful assistant. Your task is to help answer a question given in a document. The first step is to extract quotes relevant to the question from the document, delimited by ####. Please output the list of quotes using . Respond with "No relevant quotes found!" if no relevant quotes were found. #### {{document}} ####```, and the second composes an answer using those quotes and the original document, exemplified by ```Given a set of relevant quotes (delimited by ) extracted from a document and the original document (delimited by ####), please compose an answer to the question. Ensure that the answer is accurate, has a friendly tone, and sounds helpful. #### {{document}} ####```. This structured approach allows for effective transformations of responses, ultimately leading to improved user experiences.