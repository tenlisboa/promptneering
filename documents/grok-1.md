Grok-1 is a mixture-of-experts large language model with 314 billion parameters, developed by xAI, which activates 25% of its weights for each token during inference. Released under the Apache 2.0 license, Grok-1 serves as a raw base model checkpoint without fine-tuning for specific applications. It has shown strong performance in reasoning and coding tasks, achieving 63.2% on the HumanEval coding task and 73% on MMLU, outperforming ChatGPT-3.5 and Inflection-1, but still lagging behind models like GPT-4. For instance, it scored a C (59%) on the Hungarian national high school finals in mathematics, compared to GPT-4's B (68%). Due to its size, a multi-GPU machine is recommended for testing. More information can be found at "https://github.com/xai-org/grok-1".