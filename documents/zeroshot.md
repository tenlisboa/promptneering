Zero-shot prompting allows large language models (LLMs) like GPT-3.5 Turbo, GPT-4, and Claude 3 to perform tasks without examples, directly instructing the model to complete a task. For instance, a prompt such as ```Classify the text into neutral, negative or positive. Text: I think the vacation is okay. Sentiment:``` yields an output of ```Neutral```, showcasing the model's understanding of "sentiment" without prior examples. Instruction tuning enhances zero-shot learning by finetuning models on instruction-based datasets, while reinforcement learning from human feedback (RLHF) further aligns models with human preferences, as seen in ChatGPT. If zero-shot prompting fails, providing examples leads to few-shot prompting, which will be discussed in the next section.