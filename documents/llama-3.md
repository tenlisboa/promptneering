Meta has introduced Llama 3, a new family of large language models (LLMs) featuring 8B and 70B parameter pre-trained and instruction-tuned models. The architecture utilizes a standard decoder-only transformer with a vocabulary of 128K tokens, trained on sequences of 8K tokens, and incorporates grouped query attention (GQA) along with post-training techniques such as SFT, rejection sampling, PPO, and DPO. Performance-wise, Llama 3 8B outperforms Gemma 7B and Mistral 7B Instruct, while Llama 3 70 surpasses Gemini Pro 1.5 and Claude 3 Sonnet, though it slightly lags behind on the MATH benchmark. The pretrained models excel in various benchmarks like AGIEval, MMLU, and Big-Bench Hard. Additionally, a 400B parameter model is in development, with plans for multimodal support, multilingual capabilities, and longer context windows. The current checkpoint for Llama 3 400B shows promising results on common benchmarks. Licensing information is available on the model card.