Directional Stimulus Prompting, proposed by Li et al. (2023), introduces a new technique to enhance the guidance of large language models (LLMs) in generating desired summaries. This method involves training a tuneable policy language model (LM) to produce stimuli or hints, leveraging reinforcement learning to optimize LLMs. The approach allows for a smaller, optimized policy LM to generate hints that effectively guide a black-box frozen LLM, as illustrated in the comparison with standard prompting. An example of this technique will be provided soon.