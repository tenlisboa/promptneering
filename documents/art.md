The Automatic Reasoning and Tool-use (ART) framework combines chain-of-thought (CoT) prompting with tool usage to enhance task performance in large language models (LLMs). It operates by selecting multi-step reasoning demonstrations from a task library and pauses generation during tool calls to integrate their outputs before continuing. This approach allows the model to generalize and decompose new tasks in a zero-shot manner, while also enabling human intervention to correct reasoning errors or add tools. ART has shown significant improvements over few-shot prompting and automatic CoT on unseen tasks in benchmarks like BigBench and MMLU, outperforming hand-crafted CoT prompts when human feedback is included. The process is exemplified in the documentation, with performance metrics highlighted in tables.