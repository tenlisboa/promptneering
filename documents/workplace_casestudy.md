The case study by Clavi√© et al. (2023) explores prompt engineering for classifying jobs as "entry-level" using GPT-3.5 (`gpt-3.5-turbo`), which outperformed all tested models, including DeBERTa-V3. Key findings indicate that Few-shot CoT prompting was less effective than Zero-shot prompting, and the prompt significantly influenced performance, with an F1 score improvement from 65.6 to 91.7 through effective modifications. Notably, forcing the model to adhere to a template reduced performance, while small adjustments, such as giving the model a name, enhanced results. Various prompt modifications were tested, including "Baseline" for basic classification, "CoT" for examples, and "Zero-CoT" for step-by-step reasoning, with the best performance achieved through a combination of techniques, culminating in an F1 score of 91.7.