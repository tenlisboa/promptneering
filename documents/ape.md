The Automatic Prompt Engineer (APE) framework, proposed by Zhou et al. (2022), addresses the problem of automatic instruction generation and selection by framing it as a natural language synthesis task and a black-box optimization problem using large language models (LLMs). The process begins with an LLM generating instruction candidates based on output demonstrations, which are then evaluated using a target model to select the most suitable instruction. APE has been shown to discover a more effective zero-shot chain-of-thought (CoT) prompt than the human-engineered prompt "Let's think step by step," specifically the prompt "Let's work this out in a step by step way to be sure we have the right answer," which enhances performance on benchmarks like MultiArith and GSM8K. The document also references several key papers related to prompt optimization, including "Prompt-OIRL," "OPRO," "AutoPrompt," "Prefix Tuning," and "Prompt Tuning," which explore various methods for improving prompt generation and optimization.