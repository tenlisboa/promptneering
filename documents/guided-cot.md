A recent paper by Lee et al. (2024) introduces a method to enhance reasoning in large language models (LLMs) by utilizing small language models through knowledge distillation, where the small model generates rationales and the large model predicts answers, thus avoiding the need for fine-tuning the larger model. This knowledge-distilled model is further optimized with reinforcement learning using various rational-oriented and task-oriented rewards. The framework, tested on multi-hop extractive question answering, surpasses all baselines in answer prediction accuracy, with reinforcement learning improving the quality of rationales and overall performance. The proposed LM-guided Chain-of-Thought prompting method outperforms standard and Chain-of-Thought prompting, demonstrating the effective use of small models for rationale generation and encouraging developers to consider task decomposition rather than relying solely on larger models.