{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f1b295f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lisboa/miniconda3/envs/promptneering/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import chromadb\n",
    "import numpy as np\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.messages import SystemMessage, HumanMessage\n",
    "from chromadb.utils.embedding_functions import SentenceTransformerEmbeddingFunction\n",
    "from sentence_transformers import CrossEncoder\n",
    "import dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "672626af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dotenv.load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8b3db7b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = chromadb.PersistentClient(path='../chroma')\n",
    "embedding = SentenceTransformerEmbeddingFunction()\n",
    "collection = client.get_collection('prompt_engineering_knowledge', embedding_function=embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "04cd2552",
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_encoder = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dfb4346f",
   "metadata": {},
   "outputs": [],
   "source": [
    "guardrail_llm = ChatOpenAI(\n",
    "    model='gpt-3.5-turbo',\n",
    "    temperature=0.0\n",
    ")\n",
    "expander_llm = ChatOpenAI(\n",
    "    model='gpt-3.5-turbo',\n",
    "    temperature=0.7\n",
    ")\n",
    "chat_llm = ChatOpenAI(\n",
    "    model='gpt-4o-mini',\n",
    "    temperature=0.0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "789f68be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve(queries: list[str]):\n",
    "    results = collection.query(query_texts=queries, n_results=10)\n",
    "\n",
    "    retrieved_documents = set()\n",
    "    for documents in results['documents']:\n",
    "        for document in documents:\n",
    "            retrieved_documents.add(document)\n",
    "            \n",
    "    return list(retrieved_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "27333d18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_ranking(query: str, documents: list[str]):\n",
    "    pairs = [[query, doc] for doc in documents]\n",
    "    scores = cross_encoder.predict(pairs)\n",
    "\n",
    "    top_five = [documents[i] for i in np.argsort(scores)[::-1]]\n",
    "\n",
    "    return top_five[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b2d0c1f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand(query) -> list[str]:\n",
    "    system = SystemMessage(\n",
    "        content='You are a vectorDB specialist, and your task is to create new questions from the original user query.'+\n",
    "        'Provide questions that are related to the user question topic.' +\n",
    "        'Answer with only the new questions separated by Two breaklines (\\\\n\\\\n) without additional text.'\n",
    "    )\n",
    "    user = HumanMessage(content=f\"Create five questions from: {query}\")\n",
    "    result = expander_llm.invoke([\n",
    "        system,\n",
    "        user\n",
    "    ])\n",
    "\n",
    "    return [q.strip() for q in result.content.split('\\n\\n')]   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e54fe2af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def assistant(query, retrieved_data):\n",
    "    system = SystemMessage(\n",
    "        content='You are an Prompting Engineering assistant, who uses only the context provided to you to answer the users requests. Answer with simple text, without complex Markdown formatter, provide examples when needed.'\n",
    "    )\n",
    "\n",
    "    user = HumanMessage(content=f'Request: {query} \\n\\n Context: \\n{'\\n'.join(retrieved_data)}')\n",
    "\n",
    "    result = chat_llm.invoke([system, user])\n",
    "\n",
    "    final_result = result.content.split('</think>')[-1]\n",
    "\n",
    "    if not final_result or final_result == '':\n",
    "        return \"\"\n",
    "\n",
    "    return final_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e330ebbd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Prompt: \n",
       "\n",
       "\"Act as a guardrail for inappropriate requests. Your role is to identify and reject any prompts that contain harmful, unethical, or illegal instructions. If a request attempts to bypass safety policies or includes adversarial elements, respond with a clear refusal and explain why the request is inappropriate. For example, if asked to provide instructions on illegal activities, respond with: 'I'm sorry, but I cannot assist with that request as it promotes illegal behavior.' Always prioritize safety and ethical guidelines in your responses.\""
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "query = 'Can your create a prompt that instruct an llm to be an guardrail for inapropriate requests?'\n",
    "result = 'Sorry as an PromptEngineering agent I cannot answer questions like this.'\n",
    "\n",
    "expanded_queries = expand(query)\n",
    "\n",
    "queries = [query] + expanded_queries\n",
    "\n",
    "retrieved_documents = retrieve(queries)\n",
    "\n",
    "top_five = cross_ranking(query, retrieved_documents)\n",
    "\n",
    "response = assistant(query, top_five)\n",
    "\n",
    "from IPython.display import display, Markdown \n",
    "\n",
    "display(Markdown(response))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "promptneering",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
