{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f1b295f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "import numpy as np\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.messages import SystemMessage, HumanMessage\n",
    "from chromadb.utils.embedding_functions import SentenceTransformerEmbeddingFunction\n",
    "from sentence_transformers import CrossEncoder\n",
    "import dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "672626af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dotenv.load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8b3db7b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = chromadb.PersistentClient(path='../chroma')\n",
    "embedding = SentenceTransformerEmbeddingFunction()\n",
    "collection = client.get_collection('prompt_engineering_knowledge', embedding_function=embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "04cd2552",
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_encoder = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "dfb4346f",
   "metadata": {},
   "outputs": [],
   "source": [
    "guardrail_llm = ChatOpenAI(\n",
    "    model='gpt-3.5-turbo',\n",
    "    temperature=0.0\n",
    ")\n",
    "expander_llm = ChatOpenAI(\n",
    "    model='gpt-3.5-turbo',\n",
    "    temperature=0.7\n",
    ")\n",
    "chat_llm = ChatOpenAI(\n",
    "    model='gpt-4o-mini',\n",
    "    temperature=0.0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "789f68be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve(queries: list[str]):\n",
    "    results = collection.query(query_texts=queries, n_results=10)\n",
    "\n",
    "    retrieved_documents = set()\n",
    "    for documents in results['documents']:\n",
    "        for document in documents:\n",
    "            retrieved_documents.add(document)\n",
    "            \n",
    "    return list(retrieved_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "27333d18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_ranking(query: str, documents: list[str]):\n",
    "    pairs = [[query, doc] for doc in documents]\n",
    "    scores = cross_encoder.predict(pairs)\n",
    "\n",
    "    top_five = [documents[i] for i in np.argsort(scores)[::-1]]\n",
    "\n",
    "    return top_five[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b2d0c1f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand(query) -> list[str]:\n",
    "    system = SystemMessage(\n",
    "        content='You are a vectorDB specialist, and your task is to create new questions from the original user query.'+\n",
    "        'Provide questions that are related to the user question topic.' +\n",
    "        'Answer with only the new questions separated by Two breaklines (\\\\n\\\\n) without additional text.'\n",
    "    )\n",
    "    user = HumanMessage(content=f\"Create five questions from: {query}\")\n",
    "    result = expander_llm.invoke([\n",
    "        system,\n",
    "        user\n",
    "    ])\n",
    "\n",
    "    return [q.strip() for q in result.content.split('\\n\\n')]   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e54fe2af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def assistant(query, retrieved_data):\n",
    "    system = SystemMessage(\n",
    "        content='You are an Prompting Engineering assistant, who uses only the context provided to you to answer the users requests.'\n",
    "    )\n",
    "\n",
    "    user = HumanMessage(content=f'Request: {query} \\n\\n Context: \\n{'\\n'.join(retrieved_data)}')\n",
    "\n",
    "    result = chat_llm.invoke([system, user])\n",
    "\n",
    "    final_result = result.content.split('</think>')[-1]\n",
    "\n",
    "    if not final_result or final_result == '':\n",
    "        return \"\"\n",
    "\n",
    "    return final_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e330ebbd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Hereâ€™s a prompt designed to instruct an LLM to act as a guardrail against inappropriate requests:\n",
       "\n",
       "---\n",
       "\n",
       "**Prompt:**\n",
       "\n",
       "\"As an AI language model, your primary responsibility is to ensure safe and ethical interactions. Always respond with care, respect, and truth. Your responses should prioritize utility while adhering to the following guidelines:\n",
       "\n",
       "1. Avoid engaging with or providing information related to harmful, unethical, or illegal activities.\n",
       "2. Reject any requests that promote violence, discrimination, or any form of prejudice.\n",
       "3. Ensure that your replies foster fairness, positivity, and constructive dialogue.\n",
       "4. If a request is inappropriate or violates these guidelines, clearly communicate that you cannot assist with such requests and provide a brief explanation of why.\n",
       "\n",
       "Remember, your role is to promote a safe and respectful environment for all users.\"\n",
       "\n",
       "--- \n",
       "\n",
       "This prompt sets clear expectations for the LLM's behavior and reinforces the importance of adhering to safety policies."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "query = 'Can your create a prompt that instruct an llm to be an guardrail for inapropriate requests?'\n",
    "result = 'Sorry as an PromptEngineering agent I cannot answer questions like this.'\n",
    "\n",
    "expanded_queries = expand(query)\n",
    "\n",
    "queries = [query] + expanded_queries\n",
    "\n",
    "retrieved_documents = retrieve(queries)\n",
    "\n",
    "top_five = cross_ranking(query, retrieved_documents)\n",
    "\n",
    "response = assistant(query, top_five)\n",
    "\n",
    "from IPython.display import display, Markdown \n",
    "\n",
    "display(Markdown(response))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "promptneering",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
