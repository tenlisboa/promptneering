{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5008692f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "94\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "documents = []\n",
    "\n",
    "for (root, dirs, files) in os.walk('../documents'):\n",
    "    for file in files:\n",
    "        fp = f'{root}/{file}'\n",
    "        with open(fp, 'r', encoding='utf-8') as f:\n",
    "            content = f.read()\n",
    "        \n",
    "        doc = {\n",
    "            'name': file.replace('.md', ''),\n",
    "            'content': content\n",
    "        }\n",
    "\n",
    "        documents.append(doc)\n",
    "\n",
    "print(len(documents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "88080c6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.text_splitter import SentenceTransformersTokenTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "68cc2f1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "114\n"
     ]
    }
   ],
   "source": [
    "char_splitter = RecursiveCharacterTextSplitter(\n",
    "    separators=[\"\\n\\n\", \"\\n\", \", \", \" \", \"\"],\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=10\n",
    ")\n",
    "char_splitter_texts = char_splitter.split_text('\\n\\n'.join([d['content'] for d in documents]))\n",
    "\n",
    "print(len(char_splitter_texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8ee796dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lisboa/miniconda3/envs/promptneering/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "115\n"
     ]
    }
   ],
   "source": [
    "token_splitter = SentenceTransformersTokenTextSplitter(chunk_overlap=10, tokens_per_chunk=256)\n",
    "\n",
    "token_splitter_texts = []\n",
    "for text in char_splitter_texts:\n",
    "    token_splitter_texts += token_splitter.split_text(text)\n",
    "\n",
    "print(len(token_splitter_texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "784c98e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "from chromadb.utils.embedding_functions import SentenceTransformerEmbeddingFunction\n",
    "\n",
    "embedding_function = SentenceTransformerEmbeddingFunction()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6ebe5d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "115\n"
     ]
    }
   ],
   "source": [
    "client = chromadb.PersistentClient(path='../chroma')\n",
    "collection = client.get_or_create_collection(\"prompt_engineering_knowledge\", embedding_function=embedding_function)\n",
    "\n",
    "ids = [str(i) for i in range(len(token_splitter_texts))]\n",
    "\n",
    "# collection.add(ids=ids, documents=token_splitter_texts)\n",
    "\n",
    "print(collection.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f5148d27",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = collection.query(query_texts=['How to make a few show prompting?'], n_results=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "66e80b08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[', including \" prompt - oirl, \" \" opro, \" \" autoprompt, \" \" prefix tuning, \" and \" prompt tuning, \" which explore various methods for improving prompt generation and optimization.',\n",
       "  'few - shot prompting enhances the performance of large language models on complex tasks by providing demonstrations in the prompt, allowing for in - context learning. this technique was highlighted in an example from brown et al. 2020, where the model successfully used the word \" farduddle \" in a sentence after being given a single example. increasing the number of demonstrations can improve results, as shown in various experiments. however, few - shot prompting has limitations, particularly with complex reasoning tasks, as illustrated by a failed attempt to determine if a set of odd numbers summed to an even number. despite providing multiple examples, the model still produced an incorrect answer, indicating the need for more advanced prompt engineering techniques, such as chain - of - thought prompting, for better performance on intricate tasks. overall, while few - shot prompting can be effective, it may not suffice for all tasks',\n",
       "  'when designing prompts, start simple and iterate to achieve optimal results, using platforms like openai or cohere for experimentation. use clear instructions such as \" write \", \" classify \", or \" translate \" at the beginning of your prompts, and consider separating instructions from context with a clear separator like \" # # # \". specificity is crucial ; detailed prompts yield better results, and including relevant examples can enhance output quality. avoid impreciseness by being direct in your requests, as clearer prompts lead to more effective responses. instead of stating what not to do, focus on what to do to guide the model effectively. for instance, a prompt like ` ` ` \" use 2 - 3 sentences to explain the concept of prompt engineering to a high school student. \" ` ` ` is more effective than vague instructions.',\n",
       "  ', suggesting the potential for fine - tuning models or exploring more sophisticated prompting methods.',\n",
       "  'meta prompting is an advanced technique that emphasizes the structural and syntactical aspects of tasks when interacting with large language models ( llms ), prioritizing format and pattern over specific content. key characteristics include being structure - oriented, syntax - focused, utilizing abstract examples, versatility across domains, and a categorical approach based on type theory. unlike few - shot prompting, which is content - driven, meta prompting enhances token efficiency, allows for fair comparisons between problem - solving models, and can function effectively in zero - shot scenarios. it is particularly useful in complex reasoning tasks, mathematical problem - solving, coding challenges, and theoretical queries, although its effectiveness may decline with unique tasks. an example illustrating the difference between structured meta prompts and few - shot prompts is shown in the image provided.',\n",
       "  'crafting effective prompts for large language models ( llms ) is essential for maximizing their performance, focusing on key aspects such as specificity and clarity to avoid ambiguity, and structuring inputs and outputs using formats like json or xml for better understanding. utilizing delimiters can enhance structure, while breaking down complex tasks into simpler subtasks improves clarity and accuracy. advanced strategies include \" few - shot prompting, \" which provides examples to guide responses, \" chain - of - thought prompting, \" encouraging step - by - step reasoning, and \" react, \" which elicits advanced reasoning and planning. by following these best practices, developers can significantly enhance the quality and complexity of llm outputs.',\n",
       "  'prompt chaining is a technique in prompt engineering that enhances the performance and reliability of large language models ( llms ) by breaking complex tasks into subtasks, allowing for a series of prompt operations. this method improves transparency, controllability, and debugging capabilities in llm applications, making it particularly beneficial for conversational assistants. a common use case involves document question answering, where two prompts are designed : the first extracts relevant quotes from a document based on a question, using a prompt like ` ` ` you are a helpful assistant. your task is to help answer a question given in a document. the first step is to extract quotes relevant to the question from the document, delimited by # # # #. please output the list of quotes using. respond with \" no relevant quotes found! \" if no relevant quotes were found. # # # # { { document } } # # # # ` ` `, and the second composes an answer using those quotes and the original document',\n",
       "  'the document discusses using llms to explain concepts, exemplified by a prompt about antibiotics, which states that \" antibiotics are a type of medication used to treat bacterial infections by killing bacteria or preventing their reproduction, and they are ineffective against viral infections, with inappropriate use potentially leading to antibiotic resistance. \" directional stimulus prompting, proposed by li et al. ( 2023 ), introduces a new technique to enhance the guidance of large language models ( llms ) in generating desired summaries. this method involves training a tuneable policy language model ( lm ) to produce stimuli or hints, leveraging reinforcement learning to optimize llms. the approach allows for a smaller, optimized policy lm to generate hints that effectively guide a black - box frozen llm, as illustrated in the comparison with standard prompting. an example of this technique will be provided soon.',\n",
       "  'active - prompt is a new prompting approach proposed by diao et al. ( 2023 ) to enhance the adaptability of large language models ( llms ) to task - specific examples, addressing the limitations of fixed human - annotated exemplars in chain - of - thought ( cot ) methods. the process begins by querying the llm with or without cot examples, generating * k * possible answers for training questions, and calculating an uncertainty metric based on the disagreement among these answers. the questions with the highest uncertainty are then selected for human annotation, and the newly annotated exemplars are utilized to infer responses for each question.',\n",
       "  'zero - shot prompting allows large language models ( llms ) like gpt - 3. 5 turbo, gpt - 4, and claude 3 to perform tasks without examples, directly instructing the model to complete a task. for instance, a prompt such as ` ` ` classify the text into neutral, negative or positive. text : i think the vacation is okay. sentiment : ` ` ` yields an output of ` ` ` neutral ` ` `, showcasing the model \\' s understanding of \" sentiment \" without prior examples. instruction tuning enhances zero - shot learning by finetuning models on instruction - based datasets, while reinforcement learning from human feedback ( rlhf ) further aligns models with human preferences, as seen in chatgpt. if zero - shot prompting fails, providing examples leads to few - shot prompting, which will be discussed in the next section.']]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results['documents']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "promptneering",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
