{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5008692f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "94\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "documents = []\n",
    "\n",
    "for (root, dirs, files) in os.walk('documents'):\n",
    "    for file in files:\n",
    "        fp = f'{root}/{file}'\n",
    "        with open(fp, 'r', encoding='utf-8') as f:\n",
    "            content = f.read()\n",
    "        \n",
    "        doc = {\n",
    "            'name': file.replace('.md', ''),\n",
    "            'content': content\n",
    "        }\n",
    "\n",
    "        documents.append(doc)\n",
    "\n",
    "print(len(documents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "88080c6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.text_splitter import SentenceTransformersTokenTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "68cc2f1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "589\n"
     ]
    }
   ],
   "source": [
    "char_splitter = RecursiveCharacterTextSplitter(\n",
    "    separators=[\"\\n\\n\", \"\\n\", \", \", \" \", \"\"],\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=10\n",
    ")\n",
    "char_splitter_texts = char_splitter.split_text('\\n\\n'.join([d['content'] for d in documents]))\n",
    "\n",
    "print(len(char_splitter_texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8ee796dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lisboa/miniconda3/envs/promptneering/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "706\n"
     ]
    }
   ],
   "source": [
    "token_splitter = SentenceTransformersTokenTextSplitter(chunk_overlap=10, tokens_per_chunk=256)\n",
    "\n",
    "token_splitter_texts = []\n",
    "for text in char_splitter_texts:\n",
    "    token_splitter_texts += token_splitter.split_text(text)\n",
    "\n",
    "print(len(token_splitter_texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "784c98e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "from chromadb.utils.embedding_functions import SentenceTransformerEmbeddingFunction\n",
    "\n",
    "embedding_function = SentenceTransformerEmbeddingFunction()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6ebe5d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "706\n"
     ]
    }
   ],
   "source": [
    "client = chromadb.PersistentClient()\n",
    "collection = client.get_or_create_collection(\"prompt_engineering_knowledge\", embedding_function=embedding_function)\n",
    "\n",
    "ids = [str(i) for i in range(len(token_splitter_texts))]\n",
    "\n",
    "# collection.add(ids=ids, documents=token_splitter_texts)\n",
    "\n",
    "print(collection.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f5148d27",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = collection.query(query_texts=['How to make a few show prompting?'], n_results=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "66e80b08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[\"for example, you might be interested in learning the concept of prompt engineering. you might try something like : ` ` ` explain the concept prompt engineering. keep the explanation short, only a few sentences, and don ' t be too descriptive. ` ` ` it ' s not clear from the prompt above how many sentences to use and what style. you might still somewhat get good responses with the above prompts but the better prompt would be one that is very specific, concise, and to the point. something like : ` ` ` use 2 - 3 sentences to explain the concept of prompt engineering to a high school student. ` ` ` # # # to do or not to do? another common tip when designing prompts is to avoid saying what not to do but say what to do instead. this encourages more specificity and focuses on the details that lead to good responses from the model. here is an example of a movie recommendation chatbot failing at exactly what i don ' t want it to do because of how i wrote the instruction - - focusing on what not to do.\",\n",
       "  '* * task decomposition for complex operations : * * instead of presenting llms with a monolithic prompt encompassing multiple tasks, breaking down complex processes into simpler subtasks significantly improves clarity and performance. this allows the model to focus on each subtask individually, ultimately leading to a more accurate overall outcome. # # # advanced prompting strategies * * few - shot prompting : * * providing the llm with a few examples of desired input - output pairs guides it towards generating higher - quality responses by demonstrating the expected pattern. learn more about few - shot prompting [ here ] ( https : / / www. promptingguide. ai / techniques / fewshot ).',\n",
       "  \"when designing prompts, you should also keep in mind the length of the prompt as there are limitations regarding how long the prompt can be. thinking about how specific and detailed you should be. including too many unnecessary details is not necessarily a good approach. the details should be relevant and contribute to the task at hand. this is something you will need to experiment with a lot. we encourage a lot of experimentation and iteration to optimize prompts for your applications. as an example, let ' s try a simple prompt to extract specific information from a piece of text. * prompt : * ` ` ` extract the name of places in the following text. desired format : place :\",\n",
       "  'studio _ quickstart ) - [ multimodal prompts ] ( https : / / ai. google. dev / docs / multimodal _ concepts )',\n",
       "  'example of a filled - in prompt : * prompt : * ` ` ` summary : lily and timmy build a sandcastle together and learn to compromise, but it gets knocked over by a gust of wind. they find beauty in the broken sandcastle and play happily with a butterfly. features : dialogue, foreshadowing, twist sentence : one day, she went to the park and saw a beautiful butterfly. words : disagree, network, beautiful story : ` ` `',\n",
       "  '# # how to prompt gemma 7b prompting gemma 7b effectively requires being able to use the prompt template properly. in the following examples, we will cover a few examples that demonstrate the use effective use of the prompt template of gemma 7b instruct for various tasks. # # # zero - shot prompting as with any model, you can leverage gemma \\' s zero - shot capabilities by simply prompting it as follows : ` ` ` markdown user explain why the sky is blue model ` ` ` # # # zero - shot prompting with system prompt adding a system role or system prompt helps to steer llms better. while there is no explicit system role in gemma, you can add additional instructions as part of the prompt as follows : ` ` ` markdown user answer the following question in a concise and informative manner : explain why the sky is blue model ` ` ` in the example above, we added ` \" answer the following question in a concise and informative manner : \" ` as an additional instruction or system prompt to steer the model better.',\n",
       "  'effective prompt design is crucial for harnessing the full potential of llms. by adhering to best practices like specificity, structured formatting, task decomposition, and leveraging advanced techniques like few - shot, chain - of - thought, and react prompting, developers can significantly improve the quality, accuracy, and complexity of outputs generated by these powerful llms. # # # want to learn more? # # reasoning llms guide # # # table of contents',\n",
       "  \"there is no consistency in the format above but the model still predicted the correct label. we have to conduct a more thorough analysis to confirm if this holds for different and more complex tasks, including different variations of prompts. # # # limitations of few - shot prompting standard few - shot prompting works well for many tasks but is still not a perfect technique, especially when dealing with more complex reasoning tasks. let ' s demonstrate why this is the case. do you recall the previous example where we provided the following task : ` ` ` the odd numbers in this group add up to an even number : 15, 32, 5, 13, 82, 7, 1. a : ` ` ` if we try this again, the model outputs the following : ` ` ` yes, the odd numbers in this group add up to 107, which is an even number. ` ` ` this is not the correct response, which not only highlights the limitations of these systems but that there is a need for more advanced prompt engineering.\",\n",
       "  '# # # start simple as you get started with designing prompts, you should keep in mind that it is really an iterative process that requires a lot of experimentation to get optimal results. using a simple playground from openai or cohere is a good starting point. you can start with simple prompts and keep adding more elements and context as you aim for better results. iterating your prompt along the way is vital for this reason. as you read the guide, you will see many examples where specificity, simplicity, and conciseness will often give you better results. when you have a big task that involves many different subtasks, you can try to break down the task into simpler subtasks and keep building up as you get better results. this avoids adding too much complexity to the prompt design process at the beginning.',\n",
       "  '# # # the instruction you can design effective prompts for various simple tasks by using commands to instruct the model what you want to achieve, such as \" write \", \" classify \", \" summarize \", \" translate \", \" order \", etc. keep in mind that you also need to experiment a lot to see what works best. try different instructions with different keywords, contexts, and data and see what works best for your particular use case and task. usually, the more specific and relevant the context is to the task you are trying to perform, the better. we will touch on the importance of sampling and adding more context in the upcoming guides. others recommend that you place instructions at the beginning of the prompt. another recommendation is to use some clear separator like \" # # # \" to separate the instruction and context. for instance : * prompt : * ` ` ` # # # instruction # # # translate the text below to spanish : text : \" hello! \" ` ` ` * output : * ` ` ` ¡ hola! ` ` `']]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results['documents']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "promptneering",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
